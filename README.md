# AI-AutomatedImageAnnotation
create a virtual env python version == 3.10.6
install requirements.txt
create a models folder inside the project directory
download sam_vit_b_01ec64.pth inside the models folder(https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth)
download groundingdino_swint_ogc.pth inside the models folder(https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth)

# ğŸ§  Veroke AI-Powered Image Annotation Platform

**One Unified AI Platform for Image Annotation & Smart Visual Search**  
At Veroke, weâ€™re reshaping how teams and individuals interact with visual data â€” combining cutting-edge AI annotation tools with intelligent image organization and search in one seamless, web-based platform.

Whether youâ€™re labeling training data for machine learning or organizing a lifetime of digital images, our platform delivers automation, insight, and control â€” all in one place.

---

## ğŸš€ Smart AI-Powered Image Annotation â€” Built for Accuracy & Scale

Designed for AI/ML teams, researchers, and product builders, our annotation suite streamlines every step of dataset creation:

### âœ¨ Key Capabilities:
- ğŸ” **Zero-Shot Object Detection**  
  Use natural language prompts like â€œsolar panelâ€, â€œmilitary vehicleâ€, or â€œbroken windowâ€.  
  No prior training needed. Powered by **YOLOWorld** and **GroundingDINO**. (If label is in the yolo class yolo model otherwise GroundingDino)

- âœ‚ï¸ **Auto & Manual Segmentation**  
  Detected objects are segmented instantly using **SAM** or **EfficientSAM**, with optional refinement using polygons, points, or masks.

- ğŸ“ **One-Click Dataset Export**  
  Export annotations in COCO, Pascal VOC, or YOLO formats with:
  - Cropped objects
  - Organized folder structure
  - Train/val splits
  - Class label mapping

- ğŸ” **Iterative Labeling & Human-in-the-Loop Reprocessing**  
  Upload revised annotations and regenerate outputs â€” ideal for QA cycles or dataset evolution.

- ğŸ§  **CLIP-Powered Mislabel Detection**  
  We use **CLIP + UMAP + k-NN** to visualize embeddings, identify outliers, and fix class label inconsistencies *before* model training.
  (This feature is implemented in the backend but not configured at the frontend)

- ğŸ“Š **Interactive Review Dashboards**  
  Explore data using **Plotly** visualizations â€” clusters, anomalies, and dataset quality at a glance.(also only implemented in backend)

---

## ğŸ“¸ Smart Photo Gallery â€” Visual Search & Image Management

Beyond annotation, the platform includes a visual gallery for organizing and searching image collections using the same AI backbone.

### ğŸŒŸ Features:
- ğŸ“ **Text-Based Search with CLIP**  
  Query images with phrases like *â€œbeach sunset with silhouetteâ€* or *â€œred car on snowy roadâ€* â€” and retrieve semantically matched results.

- ğŸ–¼ï¸ **Reverse Image Search**  
  Upload an image and find visually similar items, regardless of filenames or folders.

- ğŸ§¹ **Duplicate & Near-Duplicate Detection**  
  Automatically clean galleries by detecting visual redundancy.

- ğŸ§© **Image Clustering**  
  Organize massive collections with unsupervised image clustering.

---
## ğŸ“¦ Project Structure
```bash
.
â”œâ”€â”€ AI-AutomatedImageAnnotation/ # FastAPI backend
â”‚ â”œâ”€â”€ main.py # FastAPI app entry point
â”‚ â”œâ”€â”€ requirements.txt # Python dependencies
â”‚ â”œâ”€â”€ Dockerfile # Backend Dockerfile
â”‚ â””â”€â”€ models/ # Stores .pth model files
â”‚ â”œâ”€â”€ sam_vit_b_01ec64.pth
â”‚ â””â”€â”€ groundingdino_swint_ogc.pth
â”‚
â”œâ”€â”€ gui/ # Angular frontend
â”‚ â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ angular.json
â”‚ â”œâ”€â”€ package.json
â”‚ â””â”€â”€ Dockerfile # Frontend Dockerfile
â”‚
â”œâ”€â”€ docker-compose.yml # Service orchestration

```
---

## ğŸš€ Features

- âœ… **Click-based segmentation** using Meta's Segment Anything Model (SAM)
- âœ… **Object detection without prompts** via Grounding DINO or YOLO models
- âœ… Docker file included
- âœ… Clean separation of backend and frontend services
- âœ… REST API for programmatic annotation workflows

---

## âš™ï¸ Requirements
Download following models in models folder:
- https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
- https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
 

For Docker(optional):
- [Docker Desktop](https://www.docker.com/products/docker-desktop)
- [WSL2](https://learn.microsoft.com/en-us/windows/wsl/install) enabled (for Windows users) 
---

## ğŸ“¥ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/Veroke-IT/AI-AutomatedImageAnnotation
cd ai-image-annotator
2. Download Model Weights
Place the following .pth files in the AI-AutomatedImageAnnotation/models/ directory:

ğŸ”— SAM ViT-B Model

ğŸ”— Grounding DINO Swin-T Model

3. Build and Run with Docker
docker compose up --build
ğŸš€ Frontend: http://localhost:4200

ğŸ§  Backend API: http://localhost:8000/docs
```

## ğŸ› ï¸ Development (Without Docker)
```bash
ğŸ”¹ Backend (FastAPI)

cd AI-AutomatedImageAnnotation
python -m venv venv
venv\Scripts\activate      # On Windows
# source venv/bin/activate # On Linux/Mac
pip install -r requirements.txt
uvicorn main:app --reload --port 8000


ğŸ”¹ Frontend (Angular)

cd gui
npm install --legacy-peer-deps
npm start

```
```bash
ğŸ“¤ API Docs
Visit Swagger UI at:
ğŸ‘‰ http://localhost:8000/docs

ğŸ§  Models Used
Segment Anything (SAM)

Grounding DINO

EfficientSAM (if integrated)

Yolo and YoloE models

Clip models
```
---
## ğŸ” License
This project is licensed under your companyâ€™s terms or an open-source license of your choice.

## ğŸ§© Troubleshooting
âŒ Docker crashes or fails to build?
Run wsl --shutdown, restart Docker Desktop, and retry docker compose up.

## âŒ Model files not found?
Make sure both .pth files are in AI-AutomatedImageAnnotation/models/.

## ğŸ‘¥ Maintainers
Built by Veroke â€” Transforming Ideas into Digital Reality

## ğŸ¤ Letâ€™s Collaborate
Weâ€™re working with AI teams, research labs, and startups to optimize their visual data workflows.

Want a demo, integration support, or early access?
## ğŸ“¬ Reach out at https://veroke.com

